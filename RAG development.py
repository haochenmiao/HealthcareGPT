# Databricks notebook source
#%pip install mlflow==2.10.1 lxml==4.9.3 transformers==4.30.2 langchain==0.1.5 databricks-vectorsearch==0.22
#dbutils.library.restartPython()

# COMMAND ----------

transcription_df = spark.sql("""
SELECT * FROM john_snow_labs_medical_transcription.medical_transcription.medical_transcription_samples
""")

# Show the first few rows of the DataFrame
transcription_df.show()
transcription_df.count()

# COMMAND ----------

from pyspark.sql.functions import udf, explode
from pyspark.sql.types import ArrayType, StringType

def chunk_text(text):
    # Check if the text is None and handle it
    if text is None:
        return []  # Return an empty list if the text is None
    # Split text into chunks by paragraph
    chunks = text.split("\n\n")  # Splits by double newlines; adjust as necessary for actual data structure
    return chunks

# Register the modified UDF in Spark
chunk_text_udf = udf(chunk_text, ArrayType(StringType()))

# Filter out rows where Transcription is None if that is acceptable
transcription_df = transcription_df.filter(transcription_df.Transcription.isNotNull())

# Or fill None values with a default string before applying the UDF
transcription_df = transcription_df.na.fill({"Transcription": "No transcription available"})

# Then apply the UDF
transcription_df = transcription_df.withColumn("text_chunks", explode(chunk_text_udf("Transcription")))
transcription_df.show()



# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS medical_transcription_chunks (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   text_chunks STRING
# MAGIC ) TBLPROPERTIES (delta.enableChangeDataFeed = true);

# COMMAND ----------

from pyspark.sql.functions import monotonically_increasing_id

# Assign IDs to each row if your DataFrame doesn't already have a unique identifier
transcription_df = transcription_df.withColumn("id", monotonically_increasing_id())

# Then apply the UDF and select the columns to write
transcription_df = transcription_df.withColumn("text_chunks", explode(chunk_text_udf("Transcription")))

# Select only the columns you want to write to the table
transcription_df = transcription_df.select("id", "text_chunks")

# Write the DataFrame to the table
transcription_df.write.format("delta").mode("overwrite").option("mergeSchema", "true").saveAsTable("medical_transcription_chunks")

# Display to confirm
display(spark.table("medical_transcription_chunks"))

# COMMAND ----------

import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

#Embeddings endpoints convert text into a vector (array of float). Here is an example using BGE:
response = deploy_client.predict(endpoint="databricks-bge-large-en", inputs={"input": ["What is Apache Spark?"]})
embeddings = [e['embedding'] for e in response.data]
print(embeddings)

# COMMAND ----------

#%pip install databricks-vectorsearch

# COMMAND ----------

# from databricks.vector_search.client import VectorSearchClient

# Instantiate the VectorSearchClient
# vsc = VectorSearchClient()

# endpoint_name = 'databricks-bge-large-en'

# Check if the endpoint already exists
# endpoints = vsc.list_endpoints()
# if endpoint_name not in endpoints:
    # Create the endpoint if it doesn't exist
    # vsc.create_endpoint(name=endpoint_name, endpoint_type="STANDARD")

# print(f"Endpoint named {endpoint_name} is ready.")

# COMMAND ----------

vsc.list_endpoints()

# COMMAND ----------

import mlflow.deployments
deploy_client = mlflow.deployments.get_deploy_client("databricks")

question = "what is the transcription of that 24 year old lady?"


results = vsc.get_index('med_transcription_vs_endpoint', 'ai_hackathon.default.med_transcript_vector_index').similarity_search(
  query_text=question,
  columns=["text_chunks"],
  num_results=1)
docs = results.get('result', {}).get('data_array', [])
docs

# COMMAND ----------

#%pip install databricks-cli
#dbutils.library.restartPython()

# COMMAND ----------

spark.sql('GRANT USAGE ON CATALOG ai_hackathon TO `SP_ID`');
spark.sql('GRANT USAGE ON DATABASE default TO `SP_ID`');

# COMMAND ----------

from databricks.sdk import WorkspaceClient
import os

# Set up environment variables for authentication
os.environ["DATABRICKS_HOST"] = 'databricks_links'
os.environ["DATABRICKS_TOKEN"] = 'access token'

# Initialize the WorkspaceClient with PAT
client = WorkspaceClient()

# COMMAND ----------


import databricks.sdk.service.catalog as c

client = WorkspaceClient(host='databricks_links', token='access token')


# COMMAND ----------

def test_demo_permissions(host, secret_scope, secret_key, vs_endpoint_name, index_name, embedding_endpoint_name):
    # Here you would add the logic to test permissions
    print(f"Testing permissions for {vs_endpoint_name} on index {index_name} with embedding {embedding_endpoint_name}")
    # Assume a function to check permissions or perform a test query
    # Actual implementation depends on what you need to verify

# Call the function with real values
test_demo_permissions('databricks_links', "healthcareRAG", "secret", 'med_transcription_vs_endpoint', 'ai_hackathon.default.med_transcript_vector_index', "databricks-bge-large-en")

# COMMAND ----------

from databricks.vector_search.client import VectorSearchClient
from langchain_community.vectorstores import DatabricksVectorSearch
from langchain_community.embeddings import DatabricksEmbeddings

# Test embedding Langchain model
#NOTE: your question embedding model must match the one used in the chunk in the previous model 
embedding_model = DatabricksEmbeddings(endpoint="databricks-bge-large-en")
print(f"Test embeddings: {embedding_model.embed_query('What is Apache Spark?')[:20]}...")

def get_retriever(persist_dir: str = None):
    os.environ["DATABRICKS_HOST"] = 'databricks_links'
    #Get the vector search index
    vsc = VectorSearchClient(workspace_url='databricks_links', personal_access_token='access token')

    index_full_name = 'ai_hackathon.default.med_transcript_vector_index'

    vs_index = vsc.get_index(
        endpoint_name='med_transcription_vs_endpoint',
        index_name=index_full_name
    )

    # Create the retriever
    vectorstore = DatabricksVectorSearch(
        vs_index, text_column="text_chunks", embedding=embedding_model
    )
    return vectorstore.as_retriever()

# COMMAND ----------

# Test Llama 3 70B LLM model
from langchain_community.chat_models import ChatDatabricks
chat_model = ChatDatabricks(endpoint="databricks-meta-llama-3-70b-instruct", max_tokens = 200)
print(f"Test chat model: {chat_model.predict('What is Apache Spark')}")

# COMMAND ----------

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatDatabricks

TEMPLATE = """You are an assistant for Databricks Healthcare users. You are answering python, coding, SQL, data engineering, spark, data science, DW and platform, API or infrastructure administration question related to Databricks. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.
Use the following pieces of context to answer the question at the end:
{context}
Question: {question}
Answer:
"""
prompt = PromptTemplate(template=TEMPLATE, input_variables=["context", "question"])

chain = RetrievalQA.from_chain_type(
    llm=chat_model,
    chain_type="stuff",
    retriever=get_retriever(),
    chain_type_kwargs={"prompt": prompt}
)

# COMMAND ----------

# langchain.debug = True #uncomment to see the chain details and the full prompt being sent
question = {"query": "what is the medical history and diagnosis with this girl who is 15 years old?"}
answer = chain.run(question)
print(answer)

# COMMAND ----------

#pip install gradio

# COMMAND ----------

# MAGIC %pip install --upgrade pydantic

# COMMAND ----------


